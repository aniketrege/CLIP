{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d510c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, datasets\n",
    "from imagenetv2_pytorch import ImageNetV2Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36294f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"../IMAGENET/\"\n",
    "OUTPUT = \"embeddings/\"\n",
    "DATASET = 'IN1K'\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "IMG_SIZE = 256\n",
    "CENTER_CROP_SIZE = 224\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "test_transform = transforms.Compose([\n",
    "transforms.Resize(IMG_SIZE),\n",
    "transforms.CenterCrop(CENTER_CROP_SIZE),\n",
    "transforms.ToTensor(),\n",
    "normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "929c0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = [\n",
    "    'RN50',\n",
    "#     'RN101',\n",
    "#     'RN50x4',\n",
    "#     'RN50x16',\n",
    "#     'RN50x64',\n",
    "#     'ViT-B/32',\n",
    "#     'ViT-B/16',\n",
    "#     'ViT-L/14',\n",
    "#     'ViT-L/14@336px'\n",
    "]\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3df2961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RN50\n"
     ]
    }
   ],
   "source": [
    "for b in backbone:\n",
    "    b = b.replace('/', '')\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1936df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataset, model):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfce25d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RN50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [21:07<00:00,  6.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1024) (50000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5005/5005 [9:05:16<00:00,  6.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1281167, 1024) (1281167,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "for b in backbone:\n",
    "    model, preprocess = clip.load(b, device)\n",
    "    train_dataset = datasets.ImageFolder(ROOT+\"train/\", transform=preprocess)\n",
    "    test_dataset = datasets.ImageFolder(ROOT+\"val/\", transform=preprocess)\n",
    "    \n",
    "    if DATASET == 'INV2':\n",
    "        test_dataset = dataset = ImageNetV2Dataset(\"matched-frequency\", transform=preprocess)\n",
    "    b = b.replace('/', '')\n",
    "    print(b)\n",
    "    \n",
    "    # Calculate the image features\n",
    "    test_features, test_labels = get_features(test_dataset, model)\n",
    "    print(test_features.shape, test_labels.shape)\n",
    "    np.save(OUTPUT+DATASET+\"_\"+str(b)+\"_\"+'test-X.npy', test_features)\n",
    "    np.save(OUTPUT+DATASET+\"_\"+str(b)+\"_\"+'test-y.npy', test_labels)\n",
    "    \n",
    "    if DATASET != 'INV2':\n",
    "        train_features, train_labels = get_features(train_dataset, model)\n",
    "        print(train_features.shape, train_labels.shape)\n",
    "        np.save(OUTPUT+DATASET+\"_\"+str(b)+\"_\"+'train-X.npy', train_features)\n",
    "        np.save(OUTPUT+DATASET+\"_\"+str(b)+\"_\"+'train-y.npy', train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ceb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform logistic regression\n",
    "# classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "# classifier.fit(train_features, train_labels)\n",
    "\n",
    "# # Evaluate using the logistic regression classifier\n",
    "# predictions = classifier.predict(test_features)\n",
    "# accuracy = np.mean((test_labels == predictions).astype(float)) * 100.\n",
    "# print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba58293",
   "metadata": {},
   "source": [
    "# CLIP k-NN Index Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_backbone = [\n",
    "    'RN50',\n",
    "#     'RN101',\n",
    "#     'RN50x4',\n",
    "#     'RN50x16',\n",
    "#     'RN50x64',\n",
    "#     'ViT-B/32',\n",
    "#     'ViT-B/16',\n",
    "#     'ViT-L/14',\n",
    "#     'ViT-L/14@336px'\n",
    "]\n",
    "\n",
    "ROOT = '../../../CLIP/embeddings/'\n",
    "DATASET = 'INV2'\n",
    "D = 512\n",
    "k = 2048\n",
    "\n",
    "for b in clip_backbone:\n",
    "    b = b.replace('/', '')\n",
    "    print(b)\n",
    "\n",
    "    database = np.load(ROOT+\"IN1K_\"+str(b)+\"_\"+'train-X.npy')\n",
    "    queries = np.load(ROOT+DATASET+\"_\"+str(b)+\"_\"+'test-X.npy')\n",
    "    \n",
    "    if not path.isdir(ROOT+'index_files/'+b):\n",
    "        makedirs(ROOT+'index_files/'+b)\n",
    "    index_file = ROOT+'index_files/'+b+'IN1K_'+str(queries.shape[1])+'dim_''exactl2.index'\n",
    "\n",
    "    # Load or build index\n",
    "    if path.exists(index_file):\n",
    "        print(\"Loading index file: \" + index_file)\n",
    "        cpu_index = faiss.read_index(index_file)\n",
    "\n",
    "    else:\n",
    "        print(\"Generating index file: \" + index_file)\n",
    "\n",
    "        xb = np.ascontiguousarray(database, dtype=np.float32)\n",
    "        faiss.normalize_L2(xb)\n",
    "        d = xb.shape[1]                           # dimension\n",
    "        nb = xb.shape[0]                       # database size\n",
    "        print(\"database: \", xb.shape)\n",
    "\n",
    "        start = time.time()\n",
    "        print(\"Building Exact L2 Index\")\n",
    "        cpu_index = faiss.IndexFlatL2(d)   # build the index\n",
    "        cpu_index.add(xb)                  # add vectors to the index\n",
    "        faiss.write_index(cpu_index, index_file)\n",
    "        print(\"GPU Index build time= %0.3f sec\" % (time.time() - start))\n",
    "    \n",
    "#     index = faiss.index_cpu_to_all_gpus(\n",
    "#         cpu_index\n",
    "#     )\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(res, 1, cpu_index)\n",
    "\n",
    "    # Load the queries\n",
    "    xq = np.ascontiguousarray(queries, dtype=np.float32)\n",
    "    faiss.normalize_L2(xq)\n",
    "    nq = xq.shape[0]\n",
    "    print(\"queries: \", xq.shape)\n",
    "\n",
    "    Dist, Ind = index.search(xq, k)\n",
    "    print(\"neighbors: \", Ind.shape)\n",
    "    if not path.isdir(ROOT+\"neighbors/\"+b):\n",
    "        makedirs(ROOT+\"neighbors/\"+b)\n",
    "    nn_dir = ROOT+\"neighbors/\"+b+\"mrl_exactl2_\"+str(database.shape[1])+\"dim_\"+str(k)+\"shortlist_\"+DATASET+\".csv\"\n",
    "    pd.DataFrame(Ind).to_csv(nn_dir, header=None, index=None)\n",
    "            \n",
    "    del index, Dist, Ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cda73d",
   "metadata": {},
   "source": [
    "# CLIP k-NN Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc8697",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLIP\n",
    "clip_backbone = [\n",
    "    'RN50',\n",
    "    'RN101',\n",
    "    'RN50x4',\n",
    "    'RN50x16',\n",
    "    'RN50x64',\n",
    "    'ViT-B/32',\n",
    "    'ViT-B/16',\n",
    "    'ViT-L/14',\n",
    "    'ViT-L/14@336px'\n",
    "]\n",
    "\n",
    "ROOT = '../../../CLIP/embeddings/'\n",
    "DATASET = 'IN1K'\n",
    "D = 512\n",
    "k = 2048\n",
    "\n",
    "for b in clip_backbone:\n",
    "    b = b.replace('/', '')\n",
    "#     print(b)\n",
    "    \n",
    "    db_labels_clip = np.load(ROOT+\"IN1K_\"+str(b)+\"_\"+'train-y.npy')\n",
    "    query_labels_clip = np.load(ROOT+DATASET+\"_\"+str(b)+\"_\"+'test-y.npy')\n",
    "    query_X = np.load(ROOT+DATASET+\"_\"+str(b)+\"_\"+'test-X.npy')\n",
    "    d = query_X.shape[1]\n",
    "    del query_X\n",
    "    \n",
    "    neighbors_path = ROOT+\"neighbors/\"+b+\"mrl_exactl2_\"+str(d)+\"dim_\"+str(k)+\"shortlist_\"+DATASET+\".csv\"\n",
    "    neighbors_clip = pd.read_csv(neighbors_path, header=None).to_numpy()\n",
    "  \n",
    "    \n",
    "    top1_clip = db_labels_clip[neighbors_clip[:, 0]]\n",
    "    print(np.sum(top1_clip == query_labels_clip) / query_labels_clip.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
